# 思考

author: tedious

data: 2023-6-15

version: 0.0.0

## 世界状态

世界状态是用户状态的总合. 为了方便查询, 为x和y建立索引.


```rust
struct Vector2 {
   x: f32,
   y: f32,
}

struct User {
   // id: u64,
   position: Vector2,
   velocity: Vector2,
   money: u64,
}

struct Users {
   inner: HashMap<u64, User>,
}

// index for x & y
struct world {
   x: BTreeMap<f32, Vec<u64> /* users with the same x */> // order by x
   y: BTreeMap<f32, Vec<u64> /* users with the same y */> // order by y
}
```

### query(min, max)

一个矩形, 所有x坐标在(min_x, max_x)之间, 且y坐标在(min_y, max_y)之间的user.

### aoe(id, radious, money)

先query( (user_x - radious), (user_y - radious) )-( (user_x + radious), (user_y + radious) )范围内的user, 再进一步筛选距离小于radious的user进行操作.

## 分析

- user大小: 8 + 8 + 8 + 8 = 32字节
- world槽位数量: 200000 * 200000 = 40_000_000_000
- 若为二维数组world_status共需要申请(40_000_000_000 * 8)B, 约为320G内存.
- 实际世界状态大小为Users大小, 假设1000w用户在线(离线用户可以移至磁盘), 共消耗(32 * 10_000_000)B, 320M内存.

> 以下均在1000w在线情况下讨论.

## [A]类似存算分离

类似MapReduce的思想. 主节点负责派发任务并汇总结果更新状态, 从节点只负责完成计算任务. 存储节点和计算节点可以分别部署集群并自由拓展, 瓶颈来自网络.

实质上是个CP系统, 除非所有Master节点全部宕机.

此方案有以下问题:

1. 如何同步状态: 若每个Step全量同步世界状态, 则相当于每秒进/出口流量都为(50 * 320)M, 约15G(120Gbit), 有点大.
   1. 可以只同步变更, 但理论上每个用户状态都会变更.
   2. 存算分离. 增加存储节点, 只有存储节点与主节点之间同步世界状态(类似数据库主从同步), 计算节点不参与世界状态的同步, 不对外暴露. 虽然这样减少了集群总网络交互, 但存储节点的压力依然没得到缓解.
   3. 想不出来有啥好办法了, 难道必须要分片???
2. 如何派发任务: 如果每次将需要计算的uid发送给节点, 相当于每秒主节点内网进/出口流量都为(50 * 320)M, 约15G(集群元数据只需要同步节点状态和用户或世界状态变更的指令, 忽略不计).
   1. 计算节点可以只计算用户的位移, 将用户的money只存放在主节点中, 不参与和计算节点的交互, 可以减少 1/4 的流量.
   2. 可以每次只在计算任务变更后派发变更的用户数据, 减少主从节点间数据交互.
   3. 主节点可以完成一部分的计算任务, 比如query可以直接计算并返回. 比如aoe, 主节点可以直接计算出变更.
3. 如何降低延迟: 0.02s即20ms更新一次, 若我每20ms向计算节点下发一次计算任务, 可能会导致延迟高于20ms(网络传输延迟).
   1. 可以计算节点任务开始后就每隔20ms上报计算结果, 减少等待. 但是可能会有个别节点网络原因导致更新不及时, 造成部分用户数据更新延迟.
4. 如何高可用:
   1. 按照设计, 拥有全量世界状态的节点参与选举, 当主节点宕机后, 重新选举主节点维护世界运行.
   2. 当全部主节点宕机后, 可以将某计算节点提升为主节点. 并从全部计算节点收集数据, 拼装处完整世界状态(若计算节点也有宕机, 则可能会不全).

## [B]按区域分片
如果按照区域来分片, 容易出现负载集中在某些区域(节点), 不能有效的负载均衡. 且用户跨区域会导致用户在服务器间转移. 不可行.

## [C]按用户分片
如果按照用户分片, 可以解决动态均衡的问题, 用户分布在各个节点上, 且用户不会出现跨服务器. 节点和节点之间

实质上是个AP系统.

问题:

1. 如果用户直接根据uid计算得出所属节点, 节点和节点间不用发起同步, 确实可以避免流量过大的问题. 但这样高可用不好做, 节点宕机后数据就丢失了.
2. 整体基于conhash或hash slot进行分配, 每个节点再做主从同步提供高可用, 当某个主节点宕机后, 从节点直接切换为主节点. 但当某个Partition的主从全部宕机, 若采用hash slot会导致整个系统不可用. 若采用conhash, 会导致数据发生丢失, 同时用户被重平衡. 同时数据交互只在每个Partition内部发生, 整体内网流量可控.
3. 丧失了区域信息, 进行aoe和query时, 需要向全节点发起操作.

## [D]按用户分片+世界状态快照(结合A和C)

一个用户的状态在速度变更之前时可预测的, 因此只需要保存一个初始状态和后续的事件日志, 即可将一个用户推进到最新状态.

所有节点通过Raft共同维护一个世界状态快照即可, 快照中保持最新的速度(更新速度时, 将快照的位置也更新到速度变更的位置)即可(还有money).

只有当更新速度时候推进此用户进度, aoe和query时将计算任务分配, 推进全局进度并返回.

## 负载均衡

我个人倾向于使用hash slot方案, 直接根据slot迁移负载即可. 但conhash实现简单, 也可以用.
